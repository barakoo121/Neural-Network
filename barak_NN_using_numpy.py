# -*- coding: utf-8 -*-
"""Barak_DL_Assignment_Done-7-12-19.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J4pfieJmFUe2jsHhlC9OKS4TRBDhK_dD
"""

import numpy as np

"""The followin class represents a simple feed forward network with multiple layers. The network class provides methods for running forward and backward for a single instance, throught the network."""

class MyNN:
  def __init__(self, learning_rate, layer_sizes):
    '''
    learning_rate - the learning to use in backward
    layer_sizes - a list of numbers, each number repreents the nuber of neurons
                  to have in every layer. Therfore, the length of the list 
                  represents the number layers this network has.
    '''
    self.learning_rate = learning_rate
    self.layer_sizes = layer_sizes
    self.model_params = {}
    self.memory = {}
    self.grads = {}
    
    # Initializing weights
    for layer_index in range(len(layer_sizes) - 1):
      W_input = layer_sizes[layer_index + 1]
      W_output = layer_sizes[layer_index]
      self.model_params['W_' + str(layer_index + 1)] = np.random.randn(W_input, W_output) * 0.1
      self.model_params['b_' + str(layer_index + 1)] = np.random.randn(W_input) * 0.1
      
      
  def forward_single_instance(self, x):    
    a_i_1 = x
    self.memory['a_0'] = x
    for layer_index in range(len(self.layer_sizes) - 1):
      W_i = self.model_params['W_' + str(layer_index + 1)]
      b_i = self.model_params['b_' + str(layer_index + 1)]
      z_i = np.dot(W_i, a_i_1) + b_i
      a_i = 1/(1+np.exp(-z_i))
      self.memory['a_' + str(layer_index + 1)] = a_i
      a_i_1 = a_i
    return a_i_1
  
  
  def log_loss(y_hat, y):
    '''
    Logistic loss, assuming a single value in y_hat and y.
    '''
    m = y_hat[0]
    cost = -y[0]*np.log(y_hat[0]) - (1 - y[0])*np.log(1 - y_hat[0])
    return cost
  
  
  def backward_single_instance(self, y):
    a_output = self.memory['a_' + str(len(self.layer_sizes) - 1)]
    dz = a_output - y #a_output is the final result after activation, y is the desired output
     
    for layer_index in range(len(self.layer_sizes) - 1, 0, -1):
      print("Layer : %s" % layer_index)
      a_l_1 = self.memory['a_' + str(layer_index - 1)] # a(L-1) output 
      dW = np.dot(dz.reshape(-1, 1), a_l_1.reshape(1, -1)) # multipling cost with a(L-1)
      self.grads['dW_' + str(layer_index)] = dW # memorizing
      W_l = self.model_params['W_' + str(layer_index)] # weights - w(L)
      dz = (a_l_1 * (1 - a_l_1)).reshape(-1, 1) * np.dot(W_l.T, dz.reshape(-1, 1))
      dB = dz
      self.grads["dB_" + str(layer_index)] = dB
  

  # update weights with grads
  def update(self): 
    for layer_index in range(len(self.layer_sizes) - 1):
      self.model_params['W_' + str(layer_index + 1)] -= self.learning_rate * self.grads['dW_' + str(layer_index + 1)]
      self.model_params['b_' + str(layer_index + 1)] -= self.learning_rate * self.grads['dB_' + str(layer_index + 1)]
  

  # X.shape = (network_input_size, number_of_instance)
  def forward_batch(self, X):
    self.memory['A_0'] = X
    A_i_1 = X
    
    for layer_index in range(len(self.layer_sizes) - 1):
      W_i = self.model_params['W_' + str(layer_index + 1)]
      b_i = self.model_params['b_' + str(layer_index + 1)]
      Z_i = np.matmul(W_i, A_i_1)
      # adding bias
      for c in Z_i.T:
            c += b_i
      A_i = 1/(1+np.exp(-Z_i))
      self.memory['A_' + str(layer_index + 1)] = A_i
      A_i_1 = A_i
    return A_i_1

  # y.shape = (1, number_of_instance)
  def backward_batch(self, Y):
    A_output = self.memory['A_' + str(len(self.layer_sizes) - 1)]
    dZ = A_output - Y
    m = len(Y) # batch size
     
    for layer_index in range(len(self.layer_sizes) - 1, 0, -1):
      A_l_1 = self.memory['A_' + str(layer_index - 1)]
      dW = (1/m)*np.matmul(dZ, A_l_1.T) 
      self.grads['dW_' + str(layer_index)] = np.sum(dW, axis=1, keepdims=True) # memorizing
      W_l = self.model_params['W_' + str(layer_index)]
      self.grads["dB_" + str(layer_index)] = (1/m)*np.sum(dZ, axis=1, keepdims=True).reshape(-1)
      dZ = (A_l_1 * (1 - A_l_1)) * np.matmul(W_l.T, dZ)   
  
  
  def log_loss_batch(self, Y_hat, Y):
    m = len(Y.reshape(-1))
    cost = sum([(1/m)*((-cur_y*np.log(cur_y_hat) - (1 - cur_y)*np.log(1 - cur_y_hat))) for cur_y, cur_y_hat in zip(Y.reshape(-1), Y_hat.reshape(-1))])
    return cost
  
  # here we assume that if the y_hat rounded equals y then we dont punish
  def log_loss_batch_boolean(self, Y_hat, Y):
    m = len(Y.reshape(-1))
    cost = sum([(1/m)*((-cur_y*np.log(cur_y_hat) - (1 - cur_y)*np.log(1 - cur_y_hat))) if np.round(cur_y_hat) != cur_y else 0 for cur_y, cur_y_hat in zip(Y.reshape(-1), Y_hat.reshape(-1))])
    return cost

nn = MyNN(0.01, [3, 2, 1])

nn.model_params

x = np.random.randn(3)
y = np.random.randn(1)

y_hat = nn.forward_single_instance(x)
print(y_hat)

nn.backward_single_instance(y)
nn.grads

def train(X, y, epochs, batch_size, loss_boolean=False):
  '''
  Train procedure, please note the TODOs inside
  '''
  loss_list = []
  for e in range(1, epochs + 1):
    epoch_loss = 0
    # shuffle
    shuffled_index = np.arange(X.shape[1])
    np.random.shuffle(shuffled_index)
    
    X_t = np.transpose(X)
    X_t = X_t[shuffled_index]
    X = np.transpose(X_t)
    
    y_t = np.transpose(y)
    y_t = y_t[shuffled_index]
    y = np.transpose(y_t)
    
    x_batches = np.array_split(X_t, batch_size)
    y_batches = np.array_split(y_t, batch_size)
    number_of_batches = len(y_batches) 

    batches = zip(x_batches, y_batches) 
    
    for X_b, y_b in batches:
      print("Batch...") 
      print("Forwarding...")
      y_hat = nn.forward_batch(X_b.T)
      print("Computing loss...")
      # only for bike sharing
      if loss_boolean:
        epoch_loss += nn.log_loss_batch_boolean(y_hat, y_b.T)
      else:
        epoch_loss += nn.log_loss_batch(y_hat, y_b.T)
      print("Backpropagating...")
      nn.backward_batch(y_b.T)
      print("Updating weights and biases...")
      nn.update()

    loss_list.append(epoch_loss/number_of_batches)
    print(f'Epoch {e}, loss={epoch_loss/number_of_batches}')

  return loss_list

nn = MyNN(0.001, [6, 4, 3, 1])

X = np.random.randn(6, 100)
y = np.random.randn(1, 100)
print(X.shape)
print(y.shape)
batch_size = 8
epochs = 10

loss_lst = train(X, y, epochs, batch_size)

# show loss
plt.plot(loss_lst)
plt.show()

"""# train on an external dataset

Training on the Bike Sharing dataset.
Using the following features from the data:

* temp
* atemp
* hum
* windspeed
* weekday

The response variable is raw["success"] = raw["cnt"] > (raw["cnt"].describe()["mean"]).
The architecture of the network is: [5, 40, 30, 10, 7, 5, 3, 1].
Use batch_size=8, training it for 100 epochs on the train set.

Then, plotting loss per epoch.
"""

import pandas as pd
from google.colab import drive
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import mean_squared_error
from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import train_test_split

drive.mount('/content/drive', force_remount=True)

raw = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Bike_Sharing.csv', usecols=['temp', 'atemp', 'hum', 'windspeed', 'weekday', 'cnt'])
raw['success'] = raw['cnt'] > (raw["cnt"].describe()["mean"])
raw = raw.drop(columns='cnt')
raw.success = raw.success.astype(int)
raw[['temp', 'atemp', 'hum', 'windspeed', 'weekday']].values.T.shape
raw['success'].values.reshape(1,-1).shape

nn = MyNN(0.001, [5, 40, 30, 10, 7, 5, 3, 1])

X = raw[['temp', 'atemp', 'hum', 'windspeed', 'weekday']].values.T
y = raw['success'].values.reshape(1,-1)

batch_size = 8
epochs = 100

loss_lst = train(X, y, epochs, batch_size, loss_boolean=True) # do not punish if round is equal

plt.plot(loss_lst)
plt.show()

